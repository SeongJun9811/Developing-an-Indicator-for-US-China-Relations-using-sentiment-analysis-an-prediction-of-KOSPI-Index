# -*- coding: utf-8 -*-
"""지표평가.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FBnO84ArKQVriMl5beGNpIBS-lKSv7MG
"""

import pandas as pd
us_c_finbert = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/지표data/US_C_indi_FInBert.csv')
kospi = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/지표data/코스피.csv')
nasdaq = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/지표data/나스닥종합.csv')
shanghai = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/지표data/상해종합.csv')
exchange = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/지표data/CNY_USD.csv')

kospi['종가'] = kospi['종가'].replace(',', '', regex=True)
kospi['종가'] = pd.to_numeric(kospi['종가'], errors='coerce')
nasdaq['종가'] = nasdaq['종가'].replace(',', '', regex=True)
nasdaq['종가'] = pd.to_numeric(nasdaq['종가'], errors='coerce')
shanghai['종가'] = shanghai['종가'].replace(',', '', regex=True)
shanghai['종가'] = pd.to_numeric(shanghai['종가'], errors='coerce')
exchange['종가'] = exchange['종가'].replace(',', '', regex=True)
exchange['종가'] = pd.to_numeric(exchange['종가'], errors='coerce')

import matplotlib.pyplot as plt

# 각 지수의 날짜 및 종가 데이터를 사용하여 선 그래프를 그린다.
plt.plot( kospi['종가'], label='KOSPI')
plt.plot( nasdaq['종가'], label='NASDAQ')
plt.plot( shanghai['종가'], label='Shanghai')
plt.plot( exchange['종가'], label='Exchange')



# 그래프에 제목과 축 레이블을 추가한다.
plt.title('Before scaling')
plt.xlabel('Date')

# 범례를 표시한다.
plt.legend()

# x 축의 눈금을 조정한다.
plt.xticks(rotation=45)

plt.savefig('stock_prices_plot.png', dpi=1500)
# 그래프를 표시한다.
plt.show()

from sklearn.preprocessing import MinMaxScaler

# MinMaxScaler를 생성한다.
scaler = MinMaxScaler()

# 각 변수에 대해 Min-Max scaling을 적용한다.
kospi['종가_scaled'] = scaler.fit_transform(kospi['종가'].values.reshape(-1, 1))
nasdaq['종가_scaled'] = scaler.fit_transform(nasdaq['종가'].values.reshape(-1, 1))
shanghai['종가_scaled'] = scaler.fit_transform(shanghai['종가'].values.reshape(-1, 1))
exchange['종가_scaled'] = scaler.fit_transform(exchange['종가'].values.reshape(-1, 1))


# 그래프를 그린다.
plt.plot( kospi['종가_scaled'], label='KOSPI')
plt.plot( nasdaq['종가_scaled'], label='NASDAQ')
plt.plot( shanghai['종가_scaled'], label='Shanghai')
plt.plot( exchange['종가_scaled'], label='Exchange')

# 그래프에 제목과 축 레이블을 추가한다.
plt.title('After scaling')
plt.xlabel('Date')

# 범례를 표시한다.
plt.legend()

# x 축의 눈금을 조정한다.
plt.xticks(rotation=45)

# 그래프를 특정 DPI로 저장한다.
plt.savefig('scaled_stock_prices_plot.png', dpi=1500)

# 그래프를 표시한다.
plt.show()

all = pd.DataFrame({'kospi' : kospi['종가'], 'nasdaq' : nasdaq['종가'], 'shanghai' : shanghai['종가'],'shanghai' : shanghai['종가']
                    ,'us_c_finbert' : us_c_finbert['score'], 'exchange' : exchange['종가'] })

all

"""LSTM

기존변수
"""

import tensorflow as tf
import numpy as np
import random
from keras.models import Sequential
from keras.layers import LSTM, Dense
from keras.callbacks import EarlyStopping

# 시드 설정
np.random.seed(2024)
tf.random.set_seed(2024)
random.seed(2024)


# 데이터 전처리
features = all[['nasdaq', 'shanghai', 'exchange']]
target = all['kospi']

# 데이터 스케일링
scaler = MinMaxScaler()
features_scaled = scaler.fit_transform(features)
target_scaled = scaler.fit_transform(target.values.reshape(-1, 1))

X, y = [], []

for i in range(len(features_scaled)):
    X.append(features_scaled[i])
    y.append(target_scaled[i])

X, y = np.array(X), np.array(y)

error = [0] * 21
pred = [0] * 21

# LOOCV
total_rows = 66
ep = [0] * 21

for i in range(45, total_rows):
    # 데이터셋 분리
    X_train, X_test = X[:i], X[i:i+1]
    y_train, y_test = y[:i], y[i:i+1]

    # 입력 데이터의 shape를 수정
    X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))
    X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))

    # LSTM 모델 생성

    model = Sequential()
    model.add(LSTM(50, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2])))
    model.add(Dense(1))
    model.compile(optimizer='adam', loss='mae')

    # 최적 에폭 설정
    early_stopping_callback = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)

    # 모델 적합
    history = model.fit(X_train, y_train, epochs=50, batch_size=32, shuffle=False,validation_data=(X_test, y_test), callbacks=[early_stopping_callback])

    # 에폭 평균 계산
    average_epochs = len(history.history['val_loss'])
    ep[i-45] = average_epochs

    # 모델 예측
    y_pred = model.predict(X_test)

    # 스케일 역변환
    y_pred_original = scaler.inverse_transform(y_pred.reshape(-1, 1))
    y_test_original = scaler.inverse_transform(y_test.reshape(-1, 1))

    pred[i-45] = y_pred_original
    error[i - 45] = y_test_original - y_pred_original

"""기존변수 + 미중관계지표"""

import tensorflow as tf
import numpy as np
import random
from keras.models import Sequential
from keras.layers import LSTM, Dense

# 시드 설정
np.random.seed(2024)
tf.random.set_seed(2024)
random.seed(2024)

# 데이터 전처리
features = all[['nasdaq', 'shanghai', 'exchange','us_c_finbert']]
target = all['kospi']

# 데이터 스케일링
scaler = MinMaxScaler()
features_scaled = scaler.fit_transform(features)
target_scaled = scaler.fit_transform(target.values.reshape(-1, 1))

X, y = [], []

for i in range(len(features_scaled)):
    X.append(features_scaled[i])
    y.append(target_scaled[i])

X, y = np.array(X), np.array(y)

error2 = [0] * 21
pred2 = [0] * 21

# LOOCV
total_rows = 66
ep2 = [0] * 21

for i in range(45, total_rows):
    # 데이터셋 분리
    X_train, X_test = X[:i], X[i:i+1]
    y_train, y_test = y[:i], y[i:i+1]

    # 입력 데이터의 shape를 수정
    X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))
    X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))

    # LSTM 모델 생성

    model = Sequential()
    model.add(LSTM(50, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2])))
    model.add(Dense(1))
    model.compile(optimizer='adam', loss='mae')

    # 최적 에폭 설정
    early_stopping_callback = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)

    # 모델 적합
    history = model.fit(X_train, y_train, epochs=50, batch_size=32, shuffle=False ,validation_data=(X_test, y_test), callbacks=[early_stopping_callback])

    # 에폭 평균 계산
    average_epochs = len(history.history['val_loss'])
    ep2[i-45] = average_epochs

    # 모델 예측
    y_pred = model.predict(X_test)

    # 스케일 역변환
    y_pred_original = scaler.inverse_transform(y_pred.reshape(-1, 1))
    y_test_original = scaler.inverse_transform(y_test.reshape(-1, 1))

    pred2[i-45] = y_pred_original
    error2[i - 45] = y_test_original - y_pred_original

rmse = np.sqrt(np.mean(np.square(error)))
rmse

mae = np.mean(np.abs(error))
mae

rmse2 = np.sqrt(np.mean(np.square(error2)))
rmse2

mae2 = np.mean(np.abs(error2))
mae2

ep

ep2

"""RNN

기존변수
"""

import tensorflow as tf
import numpy as np
import random
from keras.models import Sequential
from keras.layers import SimpleRNN, Dense
from keras.callbacks import EarlyStopping
from sklearn.preprocessing import MinMaxScaler

# 시드 설정
np.random.seed(2024)
tf.random.set_seed(2024)
random.seed(2024)

# 데이터 전처리
features = all[['nasdaq', 'shanghai', 'exchange']]
target = all['kospi']

# 데이터 스케일링
scaler = MinMaxScaler()
features_scaled = scaler.fit_transform(features)
target_scaled = scaler.fit_transform(target.values.reshape(-1, 1))

X, y = [], []

for i in range(len(features_scaled)):
    X.append(features_scaled[i])
    y.append(target_scaled[i])

X, y = np.array(X), np.array(y)

error1 = [0] * 21
pred1 = [0] * 21

# LOOCV
total_rows = 66
ep1 = [0] * 21

for i in range(45, total_rows):
    # 데이터셋 분리
    X_train, X_test = X[:i], X[i:i+1]
    y_train, y_test = y[:i], y[i:i+1]

    # 입력 데이터의 shape를 수정
    X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))
    X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))

    # SimpleRNN 모델 생성
    model = Sequential()
    model.add(SimpleRNN(50, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2])))
    model.add(Dense(1))
    model.compile(optimizer='adam', loss='mae')

    # 최적 에폭 설정
    early_stopping_callback = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)

    # 모델 적합
    history = model.fit(X_train, y_train, epochs=50, batch_size=32, shuffle=False,validation_data=(X_test, y_test), callbacks=[early_stopping_callback])

    # 에폭 평균 계산
    average_epochs = len(history.history['val_loss'])
    ep1[i-45] = average_epochs

    # 모델 예측
    y_pred = model.predict(X_test)

    # 스케일 역변환
    y_pred_original = scaler.inverse_transform(y_pred.reshape(-1, 1))
    y_test_original = scaler.inverse_transform(y_test.reshape(-1, 1))

    pred1[i-45] = y_pred_original
    error1[i - 45] = y_test_original - y_pred_original

"""기존변수 + 관계지표"""

import tensorflow as tf
import numpy as np
import random
from keras.models import Sequential
from keras.layers import SimpleRNN, Dense
from keras.callbacks import EarlyStopping
from sklearn.preprocessing import MinMaxScaler

# 시드 설정
np.random.seed(2024)
tf.random.set_seed(2024)
random.seed(2024)

# 데이터 전처리
features = all[['nasdaq', 'shanghai', 'exchange','us_c_finbert']]
target = all['kospi']

# 데이터 스케일링
scaler = MinMaxScaler()
features_scaled = scaler.fit_transform(features)
target_scaled = scaler.fit_transform(target.values.reshape(-1, 1))

X, y = [], []

for i in range(len(features_scaled)):
    X.append(features_scaled[i])
    y.append(target_scaled[i])

X, y = np.array(X), np.array(y)

error2 = [0] * 21
pred2 = [0] * 21

# LOOCV
total_rows = 66
ep2 = [0] * 21

for i in range(45, total_rows):
    # 데이터셋 분리
    X_train, X_test = X[:i], X[i:i+1]
    y_train, y_test = y[:i], y[i:i+1]

    # 입력 데이터의 shape를 수정
    X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))
    X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))

    # SimpleRNN 모델 생성
    model = Sequential()
    model.add(SimpleRNN(50, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2])))
    model.add(Dense(1))
    model.compile(optimizer='adam', loss='mae')

    # 최적 에폭 설정
    early_stopping_callback = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)

    # 모델 적합
    history = model.fit(X_train, y_train, epochs=50, batch_size=32, shuffle=False,validation_data=(X_test, y_test), callbacks=[early_stopping_callback])

    # 에폭 평균 계산
    average_epochs = len(history.history['val_loss'])
    ep2[i-45] = average_epochs

    # 모델 예측
    y_pred = model.predict(X_test)

    # 스케일 역변환
    y_pred_original = scaler.inverse_transform(y_pred.reshape(-1, 1))
    y_test_original = scaler.inverse_transform(y_test.reshape(-1, 1))

    pred2[i-45] = y_pred_original
    error2[i - 45] = y_test_original - y_pred_original

rmse1 = np.sqrt(np.mean(np.square(error1)))
rmse1

mae1 = np.mean(np.abs(error1))
mae1

rmse2 = np.sqrt(np.mean(np.square(error2)))
rmse2

mae2 = np.mean(np.abs(error2))
mae2

"""GRU

기존변수
"""

import tensorflow as tf
import numpy as np
import random
from keras.models import Sequential
from keras.layers import GRU, Dense
from keras.callbacks import EarlyStopping
from sklearn.preprocessing import MinMaxScaler

# 시드 설정
np.random.seed(2024)
tf.random.set_seed(2024)
random.seed(2024)

# 데이터 전처리
features = all[['nasdaq', 'shanghai', 'exchange']]
target = all['kospi']

# 데이터 스케일링
scaler = MinMaxScaler()
features_scaled = scaler.fit_transform(features)
target_scaled = scaler.fit_transform(target.values.reshape(-1, 1))

X, y = [], []

for i in range(len(features_scaled)):
    X.append(features_scaled[i])
    y.append(target_scaled[i])

X, y = np.array(X), np.array(y)

error1 = [0] * 21
pred1 = [0] * 21

# LOOCV
total_rows = 66
ep1 = [0] * 21

for i in range(45, total_rows):
    # 데이터셋 분리
    X_train, X_test = X[:i], X[i:i+1]
    y_train, y_test = y[:i], y[i:i+1]

    # 입력 데이터의 shape를 수정
    X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))
    X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))

    # GRU 모델 생성
    model = Sequential()
    model.add(GRU(50, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2])))
    model.add(Dense(1))
    model.compile(optimizer='adam', loss='mae')

    # 최적 에폭 설정
    early_stopping_callback = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)

    # 모델 적합
    history = model.fit(X_train, y_train, epochs=50, batch_size=32, shuffle=False,validation_data=(X_test, y_test), callbacks=[early_stopping_callback])

    # 에폭 평균 계산
    average_epochs = len(history.history['val_loss'])
    ep1[i-45] = average_epochs

    # 모델 예측
    y_pred = model.predict(X_test)

    # 스케일 역변환
    y_pred_original = scaler.inverse_transform(y_pred.reshape(-1, 1))
    y_test_original = scaler.inverse_transform(y_test.reshape(-1, 1))

    pred1[i-45] = y_pred_original
    error1[i - 45] = y_test_original - y_pred_original

"""기존변수 + 관계지표"""

import tensorflow as tf
import numpy as np
import random
from keras.models import Sequential
from keras.layers import GRU, Dense
from keras.callbacks import EarlyStopping
from sklearn.preprocessing import MinMaxScaler

# 시드 설정
np.random.seed(2024)
tf.random.set_seed(2024)
random.seed(2024)

# 데이터 전처리
features = all[['nasdaq', 'shanghai', 'exchange','us_c_finbert']]
target = all['kospi']

# 데이터 스케일링
scaler = MinMaxScaler()
features_scaled = scaler.fit_transform(features)
target_scaled = scaler.fit_transform(target.values.reshape(-1, 1))

X, y = [], []

for i in range(len(features_scaled)):
    X.append(features_scaled[i])
    y.append(target_scaled[i])

X, y = np.array(X), np.array(y)

error2 = [0] * 21
pred2 = [0] * 21

# LOOCV
total_rows = 66
ep2 = [0] * 21

for i in range(45, total_rows):
    # 데이터셋 분리
    X_train, X_test = X[:i], X[i:i+1]
    y_train, y_test = y[:i], y[i:i+1]

    # 입력 데이터의 shape를 수정
    X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))
    X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))

    # GRU 모델 생성
    model = Sequential()
    model.add(GRU(50, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2])))
    model.add(Dense(1))
    model.compile(optimizer='adam', loss='mae')

    # 최적 에폭 설정
    early_stopping_callback = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)

    # 모델 적합
    history = model.fit(X_train, y_train, epochs=50, batch_size=32, shuffle=False,validation_data=(X_test, y_test), callbacks=[early_stopping_callback])

    # 에폭 평균 계산
    average_epochs = len(history.history['val_loss'])
    ep2[i-45] = average_epochs

    # 모델 예측
    y_pred = model.predict(X_test)

    # 스케일 역변환
    y_pred_original = scaler.inverse_transform(y_pred.reshape(-1, 1))
    y_test_original = scaler.inverse_transform(y_test.reshape(-1, 1))

    pred2[i-45] = y_pred_original
    error2[i - 45] = y_test_original - y_pred_original

rmse1 = np.sqrt(np.mean(np.square(error1)))
rmse1

mae1 = np.mean(np.abs(error1))
mae1

rmse2 = np.sqrt(np.mean(np.square(error2)))
rmse2

mae2 = np.mean(np.abs(error2))
mae2
